{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypernym Relationship Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use NLTK and Hearst Pattern for hypernym relationship extraction. \n",
    "- Firstly, install python environment\n",
    "- Install NLTK: pip install nltk\n",
    "- Download data distribution for NLTK. Install using NLTK downloader: ``nltk.download()``. If cannot download using ``nltk.download()``, try download manually from https://github.com/nltk/nltk_data/tree/gh-pages![image.png](attachment:image.png) or https://pan.baidu.com/s/1wONWpaa86_wnsIksKda8eQ (code:tfon )\n",
    "- Unzip the downloaded file to the following folder: ``nltk.data.find(\".\")``\n",
    "- Unzip each zip file in the ten folders: *chunkers, corpora, grammers, help, misc, models, sentiment, stemmers, taggers, tokenizers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyponym Extraction using Hearst Pattern\n",
    "Hyponym extraction follows the following 4 steps:\n",
    "- Noun phrase chunking or named eneity chunking. You can use any np chunking/named entity technique.\n",
    "- Chunked sentences prepare. Traverse the chunked result, if the label is ``NP``, then merge all the words in this chunk and add a prefix ``NP_`` (for subsequence process).\n",
    "- Chunking refinement. If two or more NPs next to each other should be merged into a single NP. Eg., *\"NP_foo NP_bar blah blah\"* becomes *\"NP_foo_bar blah blah\"*\n",
    "- Find the hypernym and hyponym pairs based on the refined prepared chunked sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import pos_tag, word_tokenize, Tree\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expression practice: In this example, we show one regex pattern example for Hearst pattern: ``NP such as {NP,}* {(or | and)} NP`` (https://docs.python.org/3/library/re.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP_1 such as NP_2 , NP_3 and NP_4 \n"
     ]
    }
   ],
   "source": [
    "regex = r\"(NP_\\w+ (, )?such as (NP_\\w+ ?(, )?(and |or )?)+)\"\n",
    "test_str = \"NP_1 such as NP_2 , NP_3 and NP_4 \"\n",
    "matches = re.search(regex, test_str)\n",
    "if matches:\n",
    "    # Match.group([group1, ...]) Returns one or more subgroups of the match. \n",
    "    # If there is a single argument, the result is a single string;\n",
    "    # if there are multiple arguments, the result is a tuple with one item per argument. \n",
    "    # Without arguments, group1 defaults to zero (the whole match is returned).\n",
    "    print(matches.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Chunking Sentence\n",
    "- Note the result is not the chunked np, instead is the chunk tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  like/VBP\n",
      "  to/TO\n",
      "  listen/VB\n",
      "  to/TO\n",
      "  (NP music/NN)\n",
      "  from/IN\n",
      "  (NP musical/JJ genres/NNS)\n",
      "  ,/,\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  (NP blues/NNS)\n",
      "  ,/,\n",
      "  (NP rock/NN)\n",
      "  and/CC\n",
      "  (NP jazz/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "def np_chunking(sentence):\n",
    "    grammer = \"NP: {<JJ>*<NN.*>+}\\n {<NN.*>+}\"  # chunker finds any number of adjectives (JJ) and then followed by  a nouns (NN)\n",
    "    cp = nltk.RegexpParser(grammer)\n",
    "    result = cp.parse(pos_tag(word_tokenize(sentence)))\n",
    "#     result.draw()\n",
    "#     entity = []\n",
    "#     for i in result:\n",
    "#         if type(i) == Tree:\n",
    "#             ent = \" \".join([token for token, pos in i.leaves()])\n",
    "#             entity.append(ent)\n",
    "    return result\n",
    "\n",
    "print(np_chunking(\"\"\"I like to listen to music from musical genres,such as blues,rock and jazz.\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Prepare the chunked result for subsequent Hearst pattern matching\n",
    "- Traverse the chunked result, if the label is ``NP``, then merge all the words in this chunk and add a prefix ``NP_``\n",
    "- All the tokens are separated with a white space (``\" \"``) \n",
    "- Remember to lemmatize words, using ``WordNetLemmatizer`` (``from nltk.stem import WordNetLemmatizer``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the chunked sentence by merging words and add prefix NP_\n",
    "def prepare_chunks(chunks):\n",
    "        # If chunk is NP, start with NP_ and join tokens in chunk with _ ; Else just keep the token as it is\n",
    "        terms = []\n",
    "        for chunk in chunks:\n",
    "            label = None\n",
    "            try:\n",
    "                # see if the chunk is simply a word or a NP. But non-NP fail on this method call\n",
    "                label = chunk.label()\n",
    "            except:\n",
    "                pass\n",
    "            if label is None:  # means one word...\n",
    "                token = chunk[0]\n",
    "                terms.append(token)\n",
    "            else: # chunk detected\n",
    "                np = \"NP_\"+\"_\".join([WordNetLemmatizer().lemmatize(a[0]) for a in chunk])\n",
    "                if \"such\" in np:    # in pattern 3 such will be tageed as JJ, so handle this special situation\n",
    "                    np = np.replace(\"such\",\"\")\n",
    "                    terms.append(\"such\")\n",
    "                if \"other\" in np:    # in pattern 4, other will be tageed as JJ, so handle this special situation\n",
    "                    np = np.replace(\"other\",\"\")\n",
    "                    terms.append(\"other\")\n",
    "                terms.append(np)\n",
    "        return ' '.join(terms)   # use space to join every term, all the commas will be separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to listen to NP_music from NP_musical_genre , such as NP_blue , NP_rock and NP_jazz .\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"I like to listen to music from musical genres,such as blues,rock and jazz.\"\n",
    "chunk_res = np_chunking(raw_text)\n",
    "print(prepare_chunks(chunk_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Refinement chunking\n",
    "If two or more NPs next to each other should be merged into a single NP. E.g., ``NP_foo NP_bar blah blah`` becomes ``NP_foo_bar blah blah``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_NP(prepared_chunks):\n",
    "    sentence = re.sub(r\"(NP_\\w+ NP_\\w+)+\",lambda m: m.expand(r'\\1').replace(\" NP_\", \"_\"),prepared_chunks)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NP_foo_bar blah blah'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_NP(\"NP_foo NP_bar blah blah\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Find the hypernym and hyponyms on processed chunked results\n",
    "- Define Hearst patterns. Besides the regex, we also need to specify whether the hypernym is in the first part or the second part in the pattern.\n",
    "  - For example, in the pattern ``NP1 such as NP2 AND NP3``, the hypernym is the first part of the pattern; in the pattern ``NP1 , NP2 and other NP3``, the hypernym is the last part of the pattern. \n",
    "- After regex matching, find all the NPs and extract the hypernym and hyponym pairs based on the ``first`` or ``last`` attribute.\n",
    "- Clean the NPs by removing the prefix ``NP_`` and ``_``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NP_blue', 'NP_musical_genre'), ('NP_rock', 'NP_musical_genre'), ('NP_jazz', 'NP_musical_genre')]\n",
      "[('NP_basketball', 'NP__sport'), ('NP_football', 'NP__sport')]\n"
     ]
    }
   ],
   "source": [
    "# Given by the prepared text, return the hypernym-hyponym pairs\n",
    "def hyponym_extract(prepared_text, hearst_patterns):\n",
    "    pairs = []\n",
    "    for (pattern,parser) in hearst_patterns:\n",
    "        matches = re.search(pattern, prepared_text)\n",
    "        if matches:\n",
    "            match_str = matches.group(0)\n",
    "            nps = [a for a in match_str.split() if a.startswith(\"NP_\")]\n",
    "            if parser == \"first\":\n",
    "                hypernym = nps[0]\n",
    "                hyponyms = nps[1:]\n",
    "            else:\n",
    "                hypernym = nps[-1]\n",
    "                hyponyms = nps[:-1]\n",
    "            for hypo in hyponyms:\n",
    "                pairs.append((hypo,hypernym))\n",
    "    return pairs\n",
    "\n",
    "hearst_patterns = [(\"(NP_\\w+ (, )?such as (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "                       (\"((NP_\\w+ ?(, )?)+(and |or )?other NP_\\w+)\",\"last\")]\n",
    "print(hyponym_extract(prepare_chunks(np_chunking(\"I like to listen to music from musical genres,such as blues,rock and jazz.\")),hearst_patterns))\n",
    "print(hyponym_extract(prepare_chunks(np_chunking(\"He likes to play basketball,football and other sports.\")),hearst_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing, sent_tokenize, word_tokenize, and pos_tag\n",
    "# First chunking; then prepare the chunked results\n",
    "# Result is a list of prepared chunked sentences\n",
    "def prepare(chunk_patterns, raw_text):\n",
    "    sentences = nltk.sent_tokenize(raw_text.strip())\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    all_chunks = []\n",
    "    for sentence in sentences:\n",
    "        chunks = nltk.RegexpParser(chunk_patterns).parse(sentence) # chunking\n",
    "        all_chunks.append(prepare_chunks(chunks)) # prepare chunked results\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('NP_blue', 'NP_musical_genre'), ('NP_rock', 'NP_musical_genre'), ('NP_jazz', 'NP_musical_genre')]]\n",
      "[[('NP_basketball', 'NP__sport'), ('NP_football', 'NP__sport')]]\n"
     ]
    }
   ],
   "source": [
    "def find_hyponyms(sentence, hearst_patterns):\n",
    "    chunk_res = np_chunking(sentence)\n",
    "#     print(chunk_res)\n",
    "    prepare_chunk = prepare_chunks(chunk_res)\n",
    "#     print(prepare_chunk)\n",
    "    chunks_merge = merge_NP(prepare_chunk)\n",
    "#     print(chunks_merge)\n",
    "    result = []\n",
    "    pairs = hyponym_extract(chunks_merge,hearst_patterns)\n",
    "    result.append(pairs)\n",
    "    return result\n",
    "\n",
    "print(find_hyponyms(\"\"\"I like to listen to music from musical genres,such as blues,rock and jazz.\"\"\", hearst_patterns))\n",
    "print(find_hyponyms(\"\"\"He likes to play basketball,football and other sports.\"\"\",hearst_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'football'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_np(term):\n",
    "    return term.replace(\"NP_\", \"\").replace(\"_\", \" \")\n",
    "clean_np('NP_football')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Program for Hypernym extraction using Hearst Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HearstPatterns(object):\n",
    "    def __init__(self, extended = False):\n",
    "        self.__chunk_patterns = \"NP: {<JJ>*<NN.*>+} \\n {<NN.*>+}\"\n",
    "        # create a chunk parser\n",
    "        self.__np_chunker = nltk.RegexpParser(self.__chunk_patterns)\n",
    "        # create a lemmatizer to lemmatize words\n",
    "        self.__word_lemmatizer = WordNetLemmatizer()\n",
    "        # now define the Hearst patterns\n",
    "        # format is <hearst-pattern>, <hypernym_location>\n",
    "        # so, what this means is that if you apply the first pattern,\n",
    "        self.__hearst_patterns = [(\"(NP_\\w+ (, )?such as (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "                                  (\"(such NP_\\w+ as (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "                                  (\"(NP_\\w+ ?(, )?including (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n",
    "                                  (\"((NP_\\w+ ?(, )?)+(and |or )?other NP_\\w+)\",\"last\"), \n",
    "                                  (\"(NP_\\w+ ?(, )?especially (NP_\\w+ ?(, )?(and |or )?)+)\", \"first\")\n",
    "                                 ]\n",
    "\n",
    "    def getPatterns(self):\n",
    "        return self.__hearst_patterns\n",
    "\n",
    "    \n",
    "    def np_chunking(self,sentence):\n",
    "        result = self.__np_chunker.parse(sentence)\n",
    "        return result\n",
    "    \n",
    "    def prepareSentence(self, rawtext):\n",
    "        # To process text in NLTK format sentence by sentence\n",
    "        sentences = nltk.sent_tokenize(rawtext.strip())\n",
    "        sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "        sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "        return sentences\n",
    "\n",
    "    def prepare_chunks(self,chunks):\n",
    "        # If chunk is NP, start with NP_ and join tokens in chunk with _ ; Else just keep the token as it is\n",
    "        terms = []\n",
    "        for chunk in chunks:\n",
    "            label = None\n",
    "            try:\n",
    "                # see if the chunk is simply a word or a NP. But non-NP fail on this method call\n",
    "                label = chunk.label()\n",
    "            except:\n",
    "                pass\n",
    "            if label is None:  # means one word...\n",
    "                token = chunk[0]\n",
    "                terms.append(token)\n",
    "            else: # chunk detected\n",
    "                np = \"NP_\"+\"_\".join([WordNetLemmatizer().lemmatize(a[0]) for a in chunk])\n",
    "                if \"such\" in np:    # in pattern 3 such will be tageed as JJ, so handle this special situation\n",
    "                    np = np.replace(\"such\",\"\")\n",
    "                    terms.append(\"such\")\n",
    "                if \"other\" in np:    # in pattern 4, other will be tageed as JJ, so handle this special situation\n",
    "                    np = np.replace(\"other\",\"\")\n",
    "                    terms.append(\"other\")\n",
    "                terms.append(np)\n",
    "        return ' '.join(terms)   # use space to join every term, all the commas will be separated\n",
    "\n",
    "    def merge_NP(self,prepared_chunks):\n",
    "        sentence = re.sub(r\"(NP_\\w+ NP_\\w+)+\",lambda m: m.expand(r'\\1').replace(\" NP_\", \"_\"),prepared_chunks)\n",
    "        return sentence\n",
    "\n",
    "    def chunk(self, rawtext):\n",
    "        # Chunk the rawtext input\n",
    "        sentences = self.prepareSentence(rawtext.strip())\n",
    "        all_chunks = []\n",
    "        for sentence in sentences:\n",
    "            chunks = self.np_chunking(sentence)\n",
    "            all_chunks.append(self.prepare_chunks(chunks))\n",
    "\n",
    "        # two or more NPs next to each other should be merged into a single NP,\n",
    "        # find any N consecutive NP_ and merge them into one...\n",
    "        # Eg: \"NP_foo NP_bar blah blah\" becomes \"NP_foo_bar blah blah\"\n",
    "        all_prepare_chunks = []\n",
    "        for raw_chunks in all_chunks:\n",
    "            sent = self.merge_NP(raw_chunks)\n",
    "            all_prepare_chunks.append(sent)\n",
    "        return all_prepare_chunks\n",
    "    \n",
    "    def hyponym_extract(self,prepared_text, hearst_patterns):\n",
    "        pairs = []\n",
    "        for (pattern,parser) in hearst_patterns:\n",
    "            matches = re.search(pattern, prepared_text)\n",
    "            if matches:\n",
    "                match_str = matches.group(0)\n",
    "                nps = [a for a in match_str.split() if a.startswith(\"NP_\")]\n",
    "                if parser == \"first\":\n",
    "                    hypernym = nps[0]\n",
    "                    hyponyms = nps[1:]\n",
    "                else:\n",
    "                    hypernym = nps[-1]\n",
    "                    hyponyms = nps[:-1]\n",
    "                for hypo in hyponyms:\n",
    "#                     print(hypo,self.clean_np(hypo))\n",
    "                    pairs.append((self.clean_np(hypo),self.clean_np(hypernym)))\n",
    "        return pairs\n",
    "\n",
    "    \n",
    "    def find_hyponyms(self, rawtext):\n",
    "        hypo_hypernyms = []\n",
    "        pre_chunksentences = self.chunk(rawtext)\n",
    "        for sentence in pre_chunksentences:\n",
    "            pairs = self.hyponym_extract(sentence, self.getPatterns())\n",
    "            hypo_hypernyms.extend(pairs)\n",
    "        return hypo_hypernyms\n",
    "\n",
    "    def clean_np(self,term):\n",
    "        return term.replace(\"NP_\", \"\").replace(\"_\", \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gelidium', 'red algae')]\n",
      "[('Herrick', ' author'), ('Goldsmith', ' author'), ('Shakespeare', ' author')]\n",
      "[('bistro', ' cheap eating place'), ('coffee shop', ' cheap eating place')]\n",
      "[('Canada', 'common law country'), ('England', 'common law country')]\n",
      "[('France', 'European country'), ('England', 'European country'), ('Spain', 'European country')]\n"
     ]
    }
   ],
   "source": [
    "hp = HearstPatterns(extended=False)\n",
    "test = [\"Agar is a substance prepared from a mixture of red algae, such as Gelidium,for laboratory or industrial use.\",\n",
    "                         \"... works by such authors as Herrick, Goldsmith, and Shakespeare.\",\n",
    "                         \"... bistros, coffee shops, and other cheap eating places.\",\n",
    "                         \"...all common law countries, including Canada and England.\",\n",
    "                         \"...most European countries, especially France, England, and Spain.\"]\n",
    "        #         text = 'I like to listen to music from musical genres such as blues, rock and jazz. He likes to play basketball , football and other sports.'\n",
    "for txt in test:\n",
    "    hps = hp.find_hyponyms(txt)\n",
    "    print(hps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
